# OMOAI â€” Audio Processing Pipeline (ASR + Punctuation + Summarization)

A production-ready pipeline to transcribe and summarize long-form audio (e.g., podcasts). It uses the `Chunkformer` model for ASR and an LLM (via vLLM) for punctuation, capitalization, and summarization.

## Features

- **High-quality ASR**: Chunkformer-based decoding with timestamped segments.
- **Long-audio support**: Chunked decoding for multi-hour recordings.
- **Enhanced Punctuation & capitalization**: Advanced LLM-based punctuation with character-level alignment and quality metrics.
- **Quality Assessment**: Comprehensive metrics including WER, CER, PER, U-WER, and F-WER for evaluating punctuation accuracy.
- **Summarization**: Bullet points and abstract (single-pass or map-reduce for long texts).
- **Config-driven**: Single `config.yaml` controls paths, ASR, LLM, API, and outputs.
- **Outputs**: Always writes `final.json`; optionally writes transcript and summary text files. Programmable formatters available for JSON/Text/SRT/VTT/Markdown.
- **CLI & REST API**: Run locally or as a web service.

## Architecture

1. **Preprocess**: Convert input audio to 16kHz mono PCM16 WAV (ffmpeg).
2. **ASR**: Transcribe with Chunkformer, produce segments and raw transcript (`asr.json`).
3. **Post-process**: LLM adds punctuation and generates summary; write `final.json` and optional text files.

See details in `docs/architecture/index.md`.

## Requirements

- Python 3.11+
- Linux/macOS/Windows
- `ffmpeg`
- NVIDIA GPU with CUDA (recommended) for ASR and vLLM

Tip: verify `ffmpeg` with `ffmpeg -version`.

## Installation

1. Clone and enter the repo

```bash
git clone <repository-url>
cd <repository-directory>
```

1. Create a virtual env with uv and install

```bash
uv venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
uv pip install -e .
```

1. Prepare models

- Place the Chunkformer checkpoint at `models/chunkformer/chunkformer-large-vie` (or update `paths.chunkformer_checkpoint` in `config.yaml`).
- Ensure `paths.chunkformer_dir` points to `./src/chunkformer` (default) or your local Chunkformer source.

## Configuration

All settings are in `config.yaml`. Key sections:

```yaml
paths:
  chunkformer_dir: ./src/chunkformer
  chunkformer_checkpoint: ./models/chunkformer/chunkformer-large-vie
  out_dir: ./data/output

asr:
  total_batch_duration_s: 1800
  chunk_size: 64
  left_context_size: 128
  right_context_size: 128
  device: auto # auto -> cuda if available else cpu
  autocast_dtype: fp16 # on CUDA

llm:
  model_id: cpatonn/Qwen3-4B-Instruct-2507-AWQ-4bit
  quantization: auto
  max_model_len: 50000
  gpu_memory_utilization: 0.90
  max_num_seqs: 2
  max_num_batched_tokens: 512
  trust_remote_code: true # Only enable for trusted models

punctuation:
  llm: { ... } # Overrides; inherits from llm when omitted
  preserve_original_words: true
  auto_switch_ratio: 0.98
  auto_margin_tokens: 128
  adopt_case: true
  enable_paragraphs: true
  join_separator: " "
  paragraph_gap_seconds: 3.0
  # Enhanced alignment settings
  alignment:
    use_levenshtein: true # Use Levenshtein distance for word alignment
    enable_character_level: true # Enable character-level refinement
    compute_quality_metrics: true # Calculate WER, CER, PER, U-WER, F-WER
    generate_diffs: true # Generate human-readable diffs
  system_prompt: |
    <instruction>
    You are a Vietnamese text punctuation engine...
    </instruction>

summarization:
  llm: { ... } # Overrides; inherits from llm when omitted
  map_reduce: false
  auto_switch_ratio: 0.98
  auto_margin_tokens: 256
  system_prompt: |
    <instruction>
    You are a Vietnamese text analysis engine...
    </instruction>

# Legacy output controls used by scripts/post.py
write_separate_files: true
transcript_file: transcript.txt
summary_file: summary.txt
wrap_width: 0

# Optional structured output config for library usage
formats: ["json", "text", "srt", "vtt", "md"]
transcript:
  include_raw: true
  include_punct: true
  include_segments: true
  timestamps: clock
  wrap_width: 100
summary:
  mode: both
  bullets_max: 7
  abstract_max_chars: 1000
  language: vi
final_json: final.json

api:
  host: 0.0.0.0
  port: 8000
  max_body_size_mb: 100
  request_timeout_seconds: 300
  temp_dir: /tmp
  cleanup_temp_files: true
  enable_progress_output: true
```

- Use `OMOAI_CONFIG=/abs/path/to/config.yaml` to override config location at runtime.

## Quickstart (CLI)

Run the full pipeline on an audio file:

```bash
uv run start data/input/your_audio.mp3
# or
uv run python src/omoai/main.py data/input/your_audio.mp3
```

Interactive mode:

```bash
uv run interactive
# or
uv run python src/omoai/main.py --interactive
```

Outputs are written under `paths.out_dir/<audio-stem>-<UTC-timestamp>/`, e.g.:

```text
data/output/my_episode-20250101-120000/
â”œâ”€â”€ preprocessed.wav
â”œâ”€â”€ asr.json
â”œâ”€â”€ final.json
â”œâ”€â”€ transcript.txt      # if write_separate_files: true
â””â”€â”€ summary.txt         # if write_separate_files: true
```

### Stage-by-stage processing

```bash
# Using the main CLI (recommended approach)
uv run start data/input/your_audio.mp3

# Or using the Python module interface for more control
python -c "
from omoai.pipeline import run_full_pipeline_memory
result = run_full_pipeline_memory('data/input/your_audio.mp3')
print(f'Transcript: {result.transcript_punctuated}')
"
```

### Legacy script usage (deprecated)

The legacy standalone scripts have been archived to `archive/legacy_scripts/` and are no longer maintained. Please use the new pipeline modules instead. See `docs/migration_guide.md` for details on how to migrate.

## REST API

Start the server:

```bash
uv run api
# or
uv run litestar --app omoai.api.app:app run --host 0.0.0.0 --port 8000
```

Endpoints:

- `POST /pipeline` (multipart form): run full pipeline.
- `POST /preprocess` (multipart form): preprocess only.
- `POST /asr` (JSON): run ASR on a local preprocessed path.
- `POST /postprocess` (JSON): run punctuation+summary on provided ASR output.
- `GET /health`: health check (ffmpeg, config, scripts).

Examples:

```bash
# Full pipeline (default plain text response with transcript + summary)
curl -X POST 'http://localhost:8000/pipeline' \
  -F 'audio_file=@data/input/audio.mp3'

# Structured JSON with options
curl -X POST 'http://localhost:8000/pipeline?include=segments&ts=clock&summary=both&summary_bullets_max=5' \
  -F 'audio_file=@data/input/audio.mp3'

# Health
curl 'http://localhost:8000/health'
```

Notes:

- When no query params are provided to `/pipeline`, the server returns `text/plain` containing the punctuated transcript and summary for convenience.
- With query params, the server returns structured JSON (`PipelineResponse`).

## Outputs and formatters

The pipeline always writes `final.json`. With `write_separate_files: true`, it also writes `transcript.txt` and `summary.txt`.

Advanced, programmatic formatting is available via the output writer and plugins (JSON/Text/SRT/VTT/Markdown). Example:

```python
from pathlib import Path
from omoai.output.writer import write_outputs
from omoai.config.schemas import OutputConfig

config = OutputConfig(formats=["json", "text", "srt", "vtt", "md"])
written = write_outputs(
    output_dir=Path("data/output/my_run"),
    segments=segments,                 # list of {start, end, text_raw, text_punct}
    transcript_raw=transcript_raw,
    transcript_punct=transcript_punct,
    summary=summary,                   # {bullets: [...], abstract: "..."}
    metadata=metadata,                 # optional dict
    config=config,
)
print(written)
```

## Troubleshooting

- **ffmpeg not found**: Install via your OS package manager and ensure it is on PATH.
- **Model checkpoint missing**: Set `paths.chunkformer_checkpoint` to a valid local path.
- **CUDA OOM / memory issues**:
  - Lower `llm.max_num_seqs` and `llm.max_num_batched_tokens`.
  - Reduce `asr.total_batch_duration_s` or `gpu_memory_utilization`.
  - Use quantized models (e.g., AWQ) or set `quantization: auto`.
- **Security**: `trust_remote_code: true` executes remote code from model repos; only enable for trusted sources.
- **Custom config location**: `export OMOAI_CONFIG=/abs/path/config.yaml`.
- **Debug GPU cache**: set `OMOAI_DEBUG_EMPTY_CACHE=true` to hint clearing CUDA cache in some paths.

## Testing

```bash
uv run pytest
```

## Project structure

```text
.
â”œâ”€â”€ config.yaml
â”œâ”€â”€ data/
â”œâ”€â”€ models/
â”œâ”€â”€ archive/
â”‚   â””â”€â”€ legacy_scripts/
â”‚       â”œâ”€â”€ preprocess.py
â”‚       â”œâ”€â”€ asr.py
â”‚       â””â”€â”€ post.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ chunkformer/
â”‚   â””â”€â”€ omoai/
â”‚       â”œâ”€â”€ api/
â”‚       â”œâ”€â”€ output/
â”‚       â”‚   â”œâ”€â”€ formatter.py
â”‚       â”‚   â”œâ”€â”€ writer.py
â”‚       â”‚   â””â”€â”€ plugins/  # json, text, srt, vtt, md
â”‚       â”œâ”€â”€ config/
â”‚       â”œâ”€â”€ interactive_cli.py
â”‚       â””â”€â”€ main.py
â””â”€â”€ tests/
```

## Documentation

ğŸ“– **[Documentation Hub](docs/README.md)** - Streamlined documentation portal

### Essential Docs
- **[Migration Guide](docs/migration_guide.md)** - Migrating from legacy scripts
- **[Final Summary](docs/final_summary.md)** - Complete refactor summary and achievements
- **[Project Status](COMPLETED.md)** - Project completion status

### Development
- **[Architecture Overview](docs/architecture/overview.md)** - System architecture and design
- **[Development Guide](docs/development/development_guide.md)** - Contributing and development setup
- **[Testing Report](docs/development/testing_report.md)** - Test coverage and quality metrics

---

*For complete documentation index, see [docs/README.md](docs/README.md)*

## License

MIT

---

## OMOAI â€” HÆ°á»›ng dáº«n báº±ng tiáº¿ng Viá»‡t

Má»™t pipeline xá»­ lÃ½ Ã¢m thanh Ä‘á»ƒ nháº­n dáº¡ng giá»ng nÃ³i (ASR), cháº¥m cÃ¢u, vÃ  tÃ³m táº¯t ná»™i dung. Sá»­ dá»¥ng `Chunkformer` cho ASR vÃ  LLM (vLLM) Ä‘á»ƒ cháº¥m cÃ¢u vÃ  tÃ³m táº¯t.

### TÃ­nh nÄƒng

- **ASR cháº¥t lÆ°á»£ng cao** vá»›i dáº¥u thá»i gian theo Ä‘oáº¡n.
- **Há»— trá»£ audio dÃ i** (hÃ ng giá») báº±ng giáº£i mÃ£ theo khá»‘i.
- **Cháº¥m cÃ¢u & viáº¿t hoa nÃ¢ng cao**: Há»‡ thá»‘ng cháº¥m cÃ¢u thÃ´ng minh vá»›i cÄƒn chá»‰nh kÃ½ tá»± vÃ  Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng.
- **ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng**: CÃ¡c chá»‰ sá»‘ toÃ n diá»‡n bao gá»“m WER, CER, PER, U-WER, vÃ  F-WER Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a dáº¥u cÃ¢u.
- **TÃ³m táº¯t**: gáº¡ch Ä‘áº§u dÃ²ng vÃ  Ä‘oáº¡n tÃ³m táº¯t (Ä‘Æ¡n hoáº·c map-reduce cho vÄƒn báº£n dÃ i).
- **Cáº¥u hÃ¬nh táº­p trung** trong `config.yaml`.
- **Äáº§u ra**: luÃ´n cÃ³ `final.json`; tÃ¹y chá»n xuáº¥t `transcript.txt`, `summary.txt`. CÃ³ thá»ƒ táº¡o SRT/VTT/Markdown qua API thÆ° viá»‡n.
- **CLI & REST API**.

### YÃªu cáº§u

- Python 3.11+
- `ffmpeg`
- GPU NVIDIA (khuyáº¿n nghá»‹) cho tá»‘c Ä‘á»™ tá»‘t hÆ¡n

### CÃ i Ä‘áº·t

```bash
git clone <repository-url>
cd <repository-directory>
uv venv && source .venv/bin/activate
uv pip install -e .
```

Chuáº©n bá»‹ mÃ´ hÃ¬nh:

- Äáº·t checkpoint Chunkformer táº¡i `models/chunkformer/chunkformer-large-vie` hoáº·c chá»‰nh `paths.chunkformer_checkpoint`.

### Cáº¥u hÃ¬nh (`config.yaml`)

- Chá»‰nh `paths.chunkformer_dir`, `paths.chunkformer_checkpoint`, `paths.out_dir`.
- `asr.device: auto` sáº½ Æ°u tiÃªn CUDA náº¿u cÃ³.
- `llm.trust_remote_code: true` chá»‰ báº­t khi tin cáº­y nguá»“n mÃ´ hÃ¬nh.
- CÃ³ thá»ƒ Ä‘áº·t biáº¿n mÃ´i trÆ°á»ng `OMOAI_CONFIG` Ä‘á»ƒ trá» Ä‘áº¿n file cáº¥u hÃ¬nh khÃ¡c.

### Cháº¡y nhanh (CLI)

```bash
uv run start data/input/audio.mp3
# hoáº·c
uv run python src/omoai/main.py data/input/audio.mp3
```

Cháº¿ Ä‘á»™ tÆ°Æ¡ng tÃ¡c:

```bash
uv run interactive
```

ThÆ° má»¥c káº¿t quáº£ náº±m trong `data/output/<tÃªn-file>-<thá»i-gian-UTC>/`.

### REST API (Tiáº¿ng Viá»‡t)

Khá»Ÿi Ä‘á»™ng server:

```bash
uv run api
```

Gá»­i yÃªu cáº§u toÃ n bá»™ pipeline:

```bash
curl -X POST 'http://localhost:8000/pipeline' -F 'audio_file=@data/input/audio.mp3'
```

Tráº£ vá» máº·c Ä‘á»‹nh lÃ  vÄƒn báº£n (`text/plain`). Äá»ƒ nháº­n JSON cÃ³ cáº¥u trÃºc vÃ  Ä‘iá»u khiá»ƒn Ä‘áº§u ra:

```bash
curl -X POST 'http://localhost:8000/pipeline?include=segments&ts=clock&summary=both&summary_bullets_max=5' \
  -F 'audio_file=@data/input/audio.mp3'
```

Sá»©c khá»e há»‡ thá»‘ng:

```bash
curl 'http://localhost:8000/health'
```

### Äáº§u ra

- LuÃ´n cÃ³ `final.json`.
- Náº¿u `write_separate_files: true`, cÃ³ thÃªm `transcript.txt`, `summary.txt`.
- CÃ³ thá»ƒ táº¡o SRT/VTT/Markdown báº±ng API thÆ° viá»‡n (`omoai.output.writer`).

### Kháº¯c phá»¥c sá»± cá»‘

- **Thiáº¿u ffmpeg**: cÃ i Ä‘áº·t vÃ  Ä‘áº£m báº£o cÃ³ trong PATH.
- **Thiáº¿u checkpoint mÃ´ hÃ¬nh**: cáº­p nháº­t `paths.chunkformer_checkpoint`.
- **Lá»—i bá»™ nhá»›/CUDA OOM**:
  - Giáº£m `llm.max_num_seqs`, `llm.max_num_batched_tokens`.
  - Giáº£m `asr.total_batch_duration_s` hoáº·c `gpu_memory_utilization`.
  - DÃ¹ng mÃ´ hÃ¬nh Ä‘Ã£ lÆ°á»£ng tá»­ hÃ³a (AWQ), `quantization: auto`.
- **Báº£o máº­t**: chá»‰ báº­t `trust_remote_code` khi cháº¯c cháº¯n vá» nguá»“n mÃ´ hÃ¬nh.

### Kiá»ƒm thá»­

```bash
uv run pytest
```
